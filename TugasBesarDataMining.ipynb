{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tugas Besar Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Retail-Supply-Chain-Sales-Dataset.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Karakteristik Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #Menampilkan 5 baris pertama secara default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #Menampilkan informasi tentang kolom dan tipe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #Menampilkan statistik data seperti count, mean, std, min, max, dll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Row ID','Sales','Quantity','Discount','Profit']].describe() #Menampilkan statistik untuk data kategorikal/ non-numerik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Premodeling\n",
    "\n",
    "2.1 Labelling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Sales'], axis=1)\n",
    "y = df['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print('Duplikat yang ditemukan:\\n',duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_null = df.isna().sum()\n",
    "print(data_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numerik_data = df.select_dtypes(include='number')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(numerik_data.values, labels=numerik_data.columns, vert=False)\n",
    "plt.title('Boxplot Semua Kolom Outlier')\n",
    "plt.xlabel('Nilai')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_duplikasi = df[df.duplicated()]\n",
    "jumlah_duplikasi = df.duplicated().sum()\n",
    "print(\"Data yang duplikat yaitu:\\n\", data_duplikasi)\n",
    "print(\"Jumlah data duplikat adalah\", jumlah_duplikasi)\n",
    "\n",
    "missingvaluesum = df.isnull().sum()\n",
    "print(\"Jumlah nilai null pada data untuk masing-masing variabel yaitu\\n\", missingvaluesum)\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"Order Date\", \"Ship Date\", \"Order ID\",\n",
    "    \"Customer ID\", \"Customer Name\", \"Country\",\n",
    "    \"Postal Code\", \"Retail Sales People\",\n",
    "    \"Product ID\"]\n",
    "\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "print(df_cleaned.head())\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['Sub-Category (Numeric)'] = label_encoder.fit_transform(df_cleaned['Sub-Category'])\n",
    "df_cleaned['State (Numeric)'] = label_encoder.fit_transform(df_cleaned['State'])\n",
    "\n",
    "X_Produk= df_cleaned[['Sub-Category (Numeric)', 'Profit']]\n",
    "X_State = df_cleaned[['State (Numeric)', 'Sales']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Studycase 1 Segmentasi Produk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertia = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_Produk)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 3\n",
    "kmeans_produk = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_produk.fit(X_Produk)\n",
    "\n",
    "df_cleaned['Cluster_Produk'] = kmeans_produk.predict(X_Produk)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_cleaned[df_cleaned['Cluster_Produk'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data['Sub-Category (Numeric)'],\n",
    "        cluster_data['Profit'], \n",
    "        label=f'Cluster {cluster}', \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "centers_produk = kmeans_produk.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers_produk[:, 0],\n",
    "    centers_produk[:, 1], \n",
    "    c='red', \n",
    "    s=200, \n",
    "    alpha=0.9, \n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.title('K-Means Clustering (Sub-Category vs Profit)')\n",
    "plt.xlabel('Sub-Category (Numeric)')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(X_Produk, kmeans_produk.labels_)\n",
    "print(f\"Average Silhouette Score for k={optimal_k}: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_clusters': range(2, 8), \n",
    "    'init': ['k-means++', 'random'], \n",
    "    'max_iter': [300, 500, 1000] \n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_score = -1\n",
    "best_model = None\n",
    "\n",
    "for n_clusters in param_grid['n_clusters']:\n",
    "    for init in param_grid['init']:\n",
    "        for max_iter in param_grid['max_iter']:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, init=init, max_iter=max_iter, random_state=42)\n",
    "            kmeans.fit(X_Produk) \n",
    "            labels = kmeans.labels_ \n",
    "            score = silhouette_score(X_Produk, labels) \n",
    "            print(f\"n_clusters={n_clusters}, init={init}, max_iter={max_iter}, silhouette_score={score:.3f}\")\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'n_clusters': n_clusters, 'init': init, 'max_iter': max_iter}\n",
    "                best_model = kmeans\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_produk_tuning = KMeans(n_clusters=2, init='k-means++', max_iter=300, random_state=42)\n",
    "kmeans_produk_tuning.fit(X_Produk) \n",
    "\n",
    "df_cleaned['Cluster_Produk_Tuning'] = kmeans_produk_tuning.predict(X_Produk) \n",
    "\n",
    "\n",
    "silhouette_avg = silhouette_score(X_Produk, df_cleaned['Cluster_Produk_Tuning']) \n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(2):\n",
    "    cluster_data = df_cleaned[df_cleaned['Cluster_Produk_Tuning'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data['Sub-Category (Numeric)'],\n",
    "        cluster_data['Profit'], \n",
    "        label=f'Cluster {cluster}', \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "centers_produk = kmeans_produk_tuning.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers_produk[:, 0], \n",
    "    centers_produk[:, 1], \n",
    "    c='red', \n",
    "    s=200, \n",
    "    alpha=0.9, \n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.title('Clustering with Best Parameters')\n",
    "plt.xlabel('Sub-Category')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = df_cleaned[['Product Name', 'Sub-Category', 'Cluster_Produk']]\n",
    "result_table_sorted = result_table.sort_values(by='Cluster_Produk')\n",
    "result_table_sorted.to_excel('result_table_cluster_produk.xlsx', index=False)\n",
    "print(\"File berhasil disimpan sebagai 'result_table_cluster_produk.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Studycase 2. Clustering State Pelanggan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_State)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 3\n",
    "kmeans_state = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_state.fit(X_State)\n",
    "\n",
    "df_cleaned['Cluster_State'] = kmeans_state.predict(X_State)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_cleaned[df_cleaned['Cluster_State'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data['State (Numeric)'], \n",
    "        cluster_data['Sales'], \n",
    "        label=f'Cluster {cluster}', \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "centers_state = kmeans_state.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers_state[:, 0],\n",
    "    centers_state[:, 1], \n",
    "    c='red', \n",
    "    s=200, \n",
    "    alpha=0.9, \n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "\n",
    "plt.title('K-Means Clustering (State vs Sales)')\n",
    "plt.xlabel('State (Encoded)')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(X_State, kmeans_state.labels_)\n",
    "print(f\"Average Silhouette Score for k={optimal_k}: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_clusters': range(2, 8),\n",
    "    'init': ['k-means++', 'random'], \n",
    "    'max_iter': [300, 500, 1000] \n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_score = -1\n",
    "best_model = None\n",
    "\n",
    "for n_clusters in param_grid['n_clusters']:\n",
    "    for init in param_grid['init']:\n",
    "        for max_iter in param_grid['max_iter']:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, init=init, max_iter=max_iter, random_state=42)\n",
    "            kmeans.fit(X_State)\n",
    "            labels = kmeans.labels_\n",
    "            score = silhouette_score(X_State, labels)\n",
    "            print(f\"n_clusters={n_clusters}, init={init}, max_iter={max_iter}, silhouette_score={score:.3f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'n_clusters': n_clusters, 'init': init, 'max_iter': max_iter}\n",
    "                best_model = kmeans\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"Best Silhouette Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_state_tuning = KMeans(n_clusters=2, init='random', max_iter=300, random_state=42)\n",
    "kmeans_state_tuning.fit(X_State)\n",
    "\n",
    "df_cleaned['Cluster_State_Tuning'] = kmeans_state_tuning.predict(X_State)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(4):\n",
    "    cluster_data = df_cleaned[df_cleaned['Cluster_State_Tuning'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data['State (Numeric)'], \n",
    "        cluster_data['Sales'], \n",
    "        label=f'Cluster {cluster}', \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "centers = kmeans_state_tuning.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers[:, 0],\n",
    "    centers[:, 1], \n",
    "    c='red', \n",
    "    s=200, \n",
    "    alpha=0.9, \n",
    "    label='Centroids'\n",
    ")\n",
    "plt.title('K-Means Clustering (State vs Sales)')\n",
    "plt.xlabel('State (Encoded)')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(X_State, df_cleaned['Cluster_State_Tuning'])\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = df_cleaned[['State', 'Sales', 'Cluster_State']]\n",
    "result_table_sorted = result_table.sort_values(by='Cluster_State')\n",
    "result_table_sorted.to_excel('result_table_cluster_state.xlsx', index=False)\n",
    "print(\"File berhasil disimpan sebagai 'result_table_cluster_state.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Studycase 3. Prediksi Pengembalian Produk oleh Pelanggan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELING**\n",
    "\n",
    "1. Melakukan transformasi data Label Encoding untuk kolom kategorikal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score ,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"Row ID\", \"Order Date\", \"Ship Date\", \"Order ID\",\n",
    "    \"Customer ID\", \"Customer Name\", \"Country\",\n",
    "    \"Postal Code\", \"Retail Sales People\",\n",
    "    \"Product ID\", \"Product Name\"]\n",
    "\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_cleaned, columns=['Ship Mode', 'Segment', 'City', 'State', 'Region', 'Category', 'Sub-Category', 'Returned'])\n",
    "print(tabulate(df_encoded, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Melakukan pemilihan fitur yang relevan untuk model, yaitu Sales, Quantity, Discount, Ship Mode, Segment, Region, Category, Sub Category untuk yang independen (x), dan Returned untuk yang dependen (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_encoded.drop(['Returned_Yes', 'Returned_Not'], axis=1)\n",
    "y = df_encoded[['Returned_Not', 'Returned_Yes']]\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Membagi data independen dan dependen menjadi data latih dan data uji, dengan pembagian 80% train dan 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "temp_size = 0.5  # 10% / (10% + 10%)\n",
    "x_simulation, x_test, y_simulation, y_test = train_test_split(x_temp, y_temp, test_size=temp_size, random_state=42)\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Validation set size:\", x_simulation.shape)\n",
    "print(\"Test set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Melatih model dengan algoritma Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**\n",
    "\n",
    "Melakukan uji metrik terhadap model, menggunakan accuracy, precision, recall, F1-score, atau confusion matrix.\n",
    "\n",
    "\n",
    "1. Accuracy mengukur seberapa banyak prediksi model yang benar dibandingkan dengan total jumlah prediksi yang dilakukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Precision mengukur seberapa akurat model dalam memprediksi kelas positif (misalnya, produk yang dikembalikan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(y_test, y_pred, average='micro')\n",
    "print(f'Precision Score = {prec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Recall mengukur seberapa baik model dalam mendeteksi kelas positif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recall_score(y_test, y_pred, average='micro')\n",
    "print(f'Recall Score = {rec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. F1-Score adalah rata-rata harmonis antara precision dan recall, dan memberikan gambaran yang lebih seimbang antara keduanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score(y_test, y_pred, average='micro')\n",
    "print(f'f1_score = {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Confusion matrix adalah tabel yang menggambarkan hasil prediksi model dengan membandingkan nilai yang diprediksi dan nilai yang sebenarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = y_test['Returned_Yes']\n",
    "y_pred_binary = y_pred[:, 1]\n",
    "conf_mat = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "print(f'Confusion Matrix = {conf_mat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation ke Data Simulasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_simulation_pred = rf_model.predict(x_simulation)\n",
    "simulation_accuracy = accuracy_score(y_simulation, y_simulation_pred)\n",
    "print(f'Simulation Accuracy = {simulation_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "x = df_encoded.drop(['Returned_Yes', 'Returned_Not'], axis=1)\n",
    "y = df_encoded['Returned_Yes']\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "temp_size = 0.5 \n",
    "x_simulation, x_test, y_simulation, y_test = train_test_split(x_temp, y_temp, test_size=temp_size, random_state=42)\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Validation set size:\", x_simulation.shape)\n",
    "print(\"Test set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svc_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy = {accuracy}')\n",
    "\n",
    "prec = precision_score(y_test, y_pred, average='binary')\n",
    "print(f'Precision Score = {prec}')\n",
    "\n",
    "rec = recall_score(y_test, y_pred, average='binary')\n",
    "print(f'Recall Score = {rec}')\n",
    "\n",
    "f1score = f1_score(y_test, y_pred, average='binary')\n",
    "print(f'f1_score = {f1score}')\n",
    "\n",
    "y_pred_binary = (svc_model.predict_proba(x_test)[:, 1] > 0.5).astype(int)\n",
    "conf_mat = confusion_matrix(y_test, y_pred_binary)\n",
    "print(f'Confusion Matrix = {conf_mat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive-Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "x = df_encoded.drop(['Returned_Yes', 'Returned_Not'], axis=1)\n",
    "y = df_encoded['Returned_Yes']\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "temp_size = 0.5 \n",
    "x_simulation, x_test, y_simulation, y_test = train_test_split(x_temp, y_temp, test_size=temp_size, random_state=42)\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Validation set size:\", x_simulation.shape)\n",
    "print(\"Test set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy = {accuracy}')\n",
    "\n",
    "prec = precision_score(y_test, y_pred, average='binary')\n",
    "print(f'Precision Score = {prec}')\n",
    "\n",
    "rec = recall_score(y_test, y_pred, average='binary')\n",
    "print(f'Recall Score = {rec}')\n",
    "\n",
    "f1score = f1_score(y_test, y_pred, average='binary')\n",
    "print(f'f1_score = {f1score}')\n",
    "\n",
    "y_pred_binary = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred_binary)\n",
    "print(f'Confusion Matrix = {conf_mat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEPLOYMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library yg digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pickle\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyimpan Model di Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kmeansprodukmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans_produk, file)\n",
    "\n",
    "with open('kmeansstatemodel.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans_state, file)\n",
    "\n",
    "with open('rfmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisiasi Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"Row ID\", \"Order Date\", \"Ship Date\", \"Order ID\",\n",
    "    \"Customer ID\", \"Customer Name\", \"Country\",\n",
    "    \"Postal Code\", \"Retail Sales People\",\n",
    "    \"Product ID\", \"Product Name\"\n",
    "]\n",
    "\n",
    "\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['Sub-Category (Numeric)'] = label_encoder.fit_transform(df_cleaned['Sub-Category'])\n",
    "df_cleaned['State (Numeric)'] = label_encoder.fit_transform(df_cleaned['State'])\n",
    "\n",
    "X_Produk= df_cleaned[['Sub-Category (Numeric)', 'Profit']]\n",
    "X_State = df_cleaned[['State (Numeric)', 'Sales']]\n",
    "\n",
    "kmeansproduk_pipeline = Pipeline(steps=[\n",
    "    ('kmeansprodukmodel', KMeans(n_clusters=3, random_state=42))\n",
    "])\n",
    "\n",
    "kmeansproduk_pipeline.fit(X_Produk)\n",
    "df_cleaned['Cluster_Produk'] = kmeansproduk_pipeline.named_steps['kmeansprodukmodel'].predict(X_Produk)\n",
    "\n",
    "with open('kmeansproduk_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeansproduk_pipeline, file)\n",
    "\n",
    "print(\"Pipeline untuk K-Means telah dilatih dan disimpan.\")\n",
    "\n",
    "# Menentukan kolom kategorikal dan numerik\n",
    "categorical_columns = ['Ship Mode', 'Segment', 'City', 'State', 'Region', 'Category', 'Sub-Category']\n",
    "numeric_columns = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "# Memisahkan fitur dan label\n",
    "X = df_cleaned.drop(columns=['Returned'])\n",
    "y = df_cleaned['Returned']\n",
    "\n",
    "# Membagi data menjadi training, testing, dan simulasi\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "temp_size = 0.5  # Membagi data temp menjadi simulasi dan test\n",
    "X_simulation, X_test, y_simulation, y_test = train_test_split(X_temp, y_temp, test_size=temp_size, random_state=42)\n",
    "\n",
    "# Pipeline untuk preprocessing kolom kategorikal dan numerik\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ('num', StandardScaler(), numeric_columns)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Membuat pipeline untuk preprocessing dan prediksi\n",
    "rfmodel_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('rfmodel', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Melatih model dengan data training\n",
    "rfmodel_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Menyimpan pipeline yang sudah dilatih\n",
    "with open('rfmodel_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(rfmodel_pipeline, file)\n",
    "\n",
    "print(\"Model telah dilatih dan disimpan.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
